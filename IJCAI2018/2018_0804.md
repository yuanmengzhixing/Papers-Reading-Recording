# 《Watching a Small Portion could be as Good as Watching All: Towards Efficient Video Classification》 #
**IJCAI 2018**  
**RNN(GRU,LSTM) & Reinforcement**  

&emsp; 这是一篇基于深度强化学习进行视频高效率分类的文章，文章提出了一种端到端的深度强化学习方法，通过所训练的智能体（Agent)做到像人类一样 **“Watching a Small Portion could be as Good as Watching All”** 从而快速对视频进行分类,framework包含**Core network,Fast forward network,Adaptive stop network,Baseline network,Classification network**

![Framework](https://github.com/CSer-Tang-hao/Papers-Reading-Recording/blob/master/IJCAI2018/img/Fast_forward%2BAdaptive_stop.png)  
&emsp; 框架的核心思想: **Lower the number of processed frames**  
&emsp; **Motivation:** reduce the computational cost while retaining similar accuracy  
&emsp; (A)**Core network(LSTM or GRU):** (1)maintain an **internal state**（其汇总了Agent所观看的所有过去帧的历史信息）;(2)为Agent下一步该如何判断与行动提供了引导信息  
&emsp; (B)**Fast forward network:**  enhance spatio-temporal features from the **TARM**'s output  
  
![Agent](https://github.com/CSer-Tang-hao/Papers-Reading-Recording/blob/master/IJCAI2018/img/Agent.png)  
&emsp; 亮点:文章提出了一种借鉴残差网络模型结构的attention learning method，"残差"单元**TARM**由 Memory Cell、Attention和一个跨层的直连边组成，主要是用Attention Network 学到的注意力权重用于重新调整（recalibrate）原先的骨架序列的关键帧（BiGRU的输出）。对于**TARM**输出的骨架关节点**X,Y,Z**三个坐标序列，**SCTM**将其视为图片的三个通道进行再进行提取高层次的特征表示，从而更好的解决spatio-temporal variations


&emsp; PDF：https://www.ijcai.org/proceedings/2018/0098.pdf
&emsp; Code：https://github.com/hehefan/video-classification   
&emsp; Datasets: **YouTobe-8M**  
&emsp; 注：之前看过16年的一篇CVPR"End to end learning of action detection from frame glimpses in videos",是用训练好的agent去做action detection,应该是近几年CV顶会上第一篇把**RL**用到Video& Action上，发现这篇IJCAI的构思与写法都是模仿了16年的CVPR,区别在于IJCAI **limit actions to a local range**,CVPR2016 **have no constraint on the action**
