# 《Memory Attention Networks for Skeleton-based Action Recognition》 #
**IJCAI 2018**  
**CNN（ResNet） & RNN(GRU) & Attention**  

&emsp; 这是一篇基于骨架关节点进行人体动作识别的文章，文章提出了一种“RNNs+CNNs”端到端的训练框架**MANs**(Memory Attention Networks)，包含TARM(Temporal Attention Recalibration Module),STCM(Spatio-Temporal Convolution Module)  
&emsp; 输入：某个动作的   a sequence of multi-frame 3D joint coordinates，输出：action class label  
![MANs](https://github.com/CSer-Tang-hao/Papers-Reading-Recording/blob/master/IJCAI2018/img/MAN.png)  
&emsp; 框架的核心是**memory attention+convolution network**  
&emsp; **Motivation:** 进行骨架关节点的complex spatio-temporal variations建模  
&emsp; **TARM:** （1）Memory cell :use **BiGRU** to extract memory information features ;(2) Attention : learning temporal attention for feature recalibration  
&emsp; **STCM:**  enhance spatio-temporal features from the **TARM**'s output  
  
![TARM](https://github.com/CSer-Tang-hao/Papers-Reading-Recording/blob/master/IJCAI2018/img/TARM.png)  
&emsp; 亮点:文章提出了一种借鉴残差网络模型结构的attention learning method，"残差"单元**TARM**由 Memory Cell、Attention和一个跨层的直连边组成，主要是用Attention Network 学到的注意力权重用于重新调整（recalibrate）原先的骨架序列的关键帧（BiGRU的输出）。对于**TARM**输出的骨架关节点**X,Y,Z**三个坐标序列，**SCTM**将其视为图片的三个通道进行再进行提取高层次的特征表示，从而更好的解决spatio-temporal variations


&emsp; ArXiv：https://arxiv.org/abs/1804.08254  
&emsp; Code：https://github.com/memory-attention-networks （暂时未放出）  
&emsp; Datasets: **NTU RGB+D**  
&emsp; 注：不太熟悉RNN，对BiGRU部分的具体实施细节还没有整明白，Experiments Section发现这篇文章就是16,17年各种SOTA modules拼凑出来的，主要有ResNet  ,Wide-ResNet,DenseNet, BiGRU et al.
