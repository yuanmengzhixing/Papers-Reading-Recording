<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
# 《Watching a Small Portion could be as Good as Watching All: Towards Efficient Video Classification》 #
**IJCAI 2018**  
**RNNs(GRU,LSTM) & Reinforcement**  

&emsp; 这是一篇基于深度强化学习进行视频高效率分类的文章，文章提出了一种端到端的深度强化学习方法，通过所训练的智能体（Agent）做到像人类一样 **“Watching a Small Portion could be as Good as Watching All”** 从而快速对视频进行分类,其framework包含**Core network**(图A),**Fast forward network**(图B),**Adaptive stop network**(图C),**Baseline network**(图A),**Classification network**(图A)

![Framework](https://github.com/CSer-Tang-hao/Papers-Reading-Recording/blob/master/IJCAI2018/img/Fast_forward%2BAdaptive_stop.png)  
&emsp; 提高分类效率的核心思想: **Lower the number of processed frames**  
&emsp; **Motivation:** reduce the computational cost while retaining similar accuracy  
&emsp; (A)**Core network(LSTM or GRU):** (1)maintain an **internal state**（其汇总了Agent所观看的所有过去帧的历史信息）;(2)为Agent下一步该如何判断与行动提供了引导信息  
&emsp; (B)**Fast forward network:** 在观察一帧（由CNN输出的frame feature）之后，Agent通过从<a href="https://www.codecogs.com/eqnedit.php?latex=A\left&space;\{&space;-2s,-1s,&plus;1s,&plus;2s,&plus;4s,&plus;8s,&plus;16s&space;\right&space;\}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?A\left&space;\{&space;-2s,-1s,&plus;1s,&plus;2s,&plus;4s,&plus;8s,&plus;16s&space;\right&space;\}" title="A\left \{ -2s,-1s,+1s,+2s,+4s,+8s,+16s \right \}" /></a>中选择动作<a href="https://www.codecogs.com/eqnedit.php?latex=a_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a_{t}" title="a_{t}" /></a>来确定在其下一步观看的位置（可快进或慢退 ）
&emsp; (B)**Adaptive stop network:** 
&emsp; (B)**Baseline network:** 
&emsp; (B)**Classification network:** 
![Agent](https://github.com/CSer-Tang-hao/Papers-Reading-Recording/blob/master/IJCAI2018/img/Agent.png)  
&emsp; 亮点:文章提出了一种借鉴残差网络模型结构的attention learning method，"残差"单元**TARM**由 Memory Cell、Attention和一个跨层的直连边组成，主要是用Attention Network 学到的注意力权重用于重新调整（recalibrate）原先的骨架序列的关键帧（BiGRU的输出）。对于**TARM**输出的骨架关节点**X,Y,Z**三个坐标序列，**SCTM**将其视为图片的三个通道进行再进行提取高层次的特征表示，从而更好的解决spatio-temporal variations


&emsp; PDF：https://www.ijcai.org/proceedings/2018/0098.pdf
&emsp; Code：https://github.com/hehefan/video-classification   
&emsp; Datasets: **YouTobe-8M**  
&emsp; 注：之前看过16年的一篇CVPR"End to end learning of action detection from frame glimpses in videos",是用训练好的agent去做action detection,应该是近几年CV顶会上第一篇把**RL**用到Video& Action上，发现这篇IJCAI的构思与写法都是模仿了16年的CVPR,区别在于IJCAI **limit actions to a local range**,CVPR2016 **have no constraint on the action**
