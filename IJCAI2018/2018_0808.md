<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
# 《Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation》 #
**IJCAI 2018**  
### 什么是“Co-occurrence”？


&emsp; 人的某个行为动作常常和骨架的一些特定关节点构成的集合，以及这个集合中节点的交互密切相关。如要判别是否在打电话，关节点“手腕”、“手肘”、“肩膀”和“头”的动作最为关键。不同的行为动作与之密切相关的节点集合有所不同，如对于“走路”的行为动作，“脚腕”、“膝盖”、“臀部”等关节点构成具有判别力的节点集合。**我们将这种几个关节点同时影响和决定判别的特性称为共现性（Co-occurrence）**  

&emsp; 海康威视这篇文章做的事情是从人体关键点序列进行行为检测和识别,骨架的相互作用和组合在描述动作特征上共同发挥了关键性作用。文中主要提出了全局共现特征的一种提取方法,即通过转置关键点序列,将被卷积的通道维度由坐标维度变为关键点维度,从而提取关键点的全局共现特征;除了全局共现特征,文中还提到了关于多人交互动作问题的解决方法;最后也借鉴Faster RCNN的proposal思想,提出了一种时域动作检测方法.
### 局部（点）和全局卷积特征
![Decomposition](https://github.com/CSer-Tang-hao/Papers-Reading-Recording/blob/master/IJCAI2018/img/Decomposition.png)  
&emsp; 尽管 LSTM 网络就是为建模长期的时间依赖关系而设计的，但由于时间建模是在原始输入空间上完成的，所以它们难以直接从骨架上学习到高层面的特征.而全连接层则有能力聚合所有输入神经元的全局信息，进而可以学习到共现特征.文章设计了一种全新的端到端分层式特征学习网络，其中的特征是从点层面特征到全局共现特征逐渐聚合起来的  
&emsp; **3×3 卷积的分解分为两个步骤:**  
&emsp; (a) 每个输入通道的空间域中的独立 2D 卷积，其中的特征是从 3×3 的临近区域局部聚合的 
&emsp; (b) 各个通道上逐个元素求和，其中的特征是在所有输入通道上全局地聚合  
&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=T\times&space;N\times&space;D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?T\times&space;N\times&space;D" title="T\times N\times D" /></a> 的Feature Map过两个卷积层,卷积层参数分别是<a href="https://www.codecogs.com/eqnedit.php?latex=\left&space;(&space;1\times1\times64&space;\right&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\left&space;(&space;1\times1\times64&space;\right&space;)" title="\left ( 1\times1\times64 \right )" /></a>和<a href="https://www.codecogs.com/eqnedit.php?latex=\left&space;(&space;3\times1\times32&space;\right&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\left&space;(&space;3\times1\times32&space;\right&space;)" title="\left ( 3\times1\times32 \right )" /></a>. Feature Map结构为<a href="https://www.codecogs.com/eqnedit.php?latex=\left&space;(H,W,C&space;\right&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\left&space;(H,W,C&space;\right&space;)" title="\left (H,W,C \right )" /></a>,卷积核<a href="https://www.codecogs.com/eqnedit.php?latex=\left&space;(1\times1&space;\right&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\left&space;(1\times1&space;\right&space;)" title="\left (1\times1 \right )" /></a>和<a href="https://www.codecogs.com/eqnedit.php?latex=\left&space;(3\times1&space;\right&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\left&space;(3\times1&space;\right&space;)" title="\left (3\times1 \right )" /></a>分别是关键点级(point-level)的特征和3帧时序上的关键点级特征.  
&emsp; 通过tranpose(permute)操作,按(0,2,1)将Feature Map转为<a href="https://www.codecogs.com/eqnedit.php?latex=T\times&space;D\times&space;N" target="_blank"><img src="https://latex.codecogs.com/gif.latex?T\times&space;D\times&space;N" title="T\times D\times N" /></a>顺序.使用两层<a href="https://www.codecogs.com/eqnedit.php?latex=3\times&space;3" target="_blank"><img src="https://latex.codecogs.com/gif.latex?3\times&space;3" title="3\times 3" /></a>卷积核作用其上(原文中filter数分别为32和64,stride为2),N个关键点作为channel,卷积层可提取所有关键点间的全局特征.<a href="https://www.codecogs.com/eqnedit.php?latex=3\times&space;3" target="_blank"><img src="https://latex.codecogs.com/gif.latex?3\times&space;3" title="3\times 3" /></a>卷积核在对应3帧和3个维度的坐标上.  
### 多人问题  
&emsp; **为什么？**  
&emsp; 部分动作如拥抱、握手等，是多人参与的. 全面地使用了多人特征融合策略，让网络可以轻松地扩展用于人数不同的场景  
&emsp; **怎么做？**  
&emsp; **早期融合:** 多人的关键点信息在输入网络前进行stack,设置最大可接受人数,人数不足时,zero padding  
&emsp; **后期融合:** 多人的关键点信息分别输入网络,取conv6的特征,进行Max,Mean或Concat操作后,再进行分类
![Multi-person](https://github.com/CSer-Tang-hao/Papers-Reading-Recording/blob/master/IJCAI2018/img/Multi-person.png)  

&emsp; **亮点**:基于骨架关键点的人体动作识别任务的最关键因素在于两方面：用于关节共现的帧内表征和用于骨架的时间演化的帧间表征.本论文提出了一种端到端的卷积式共现特征学习框架，这些共现特征是用一种分层式的方法学习到的，其中不同层次的环境信息（contextual information）是逐渐聚合的.首先独立地编码每个节点的点层面的信息，然后同时在空间域和时间域将它们组合成形义表征，具体而言是一种全局空间聚合方案，可以学习到优于局部聚合方法的关节共现特征。此外，作者还将原始的骨架坐标与它们的时间差异整合成了一种双流式的范式。

&emsp; ArXiv：https://arxiv.org/abs/1804.06055  
&emsp; Code：https://github.com/fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition （Keras）  
&emsp; Code：https://github.com/zhujiagang/co-cn （Pytorch）  
&emsp; Datasets: **NTU RGB+D、SBU Kinect Interaction、PKU-MMD**  
&emsp; 注：这篇文章读了很长时间，但还是半懂状态，为什么这种方法会work,文章很多细节的原理并没读懂，感觉自己对CNN的理解还不够深，官方code也没放出，后续还要再研究一下这种分层聚合的方法。
