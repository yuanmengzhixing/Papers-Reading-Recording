<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
# 《Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation》 #
**IJCAI 2018**  
### 什么是“Co-occurrence”？


&emsp; 人的某个行为动作常常和骨架的一些特定关节点构成的集合，以及这个集合中节点的交互密切相关。如要判别是否在打电话，关节点“手腕”、“手肘”、“肩膀”和“头”的动作最为关键。不同的行为动作与之密切相关的节点集合有所不同，如对于“走路”的行为动作，“脚腕”、“膝盖”、“臀部”等关节点构成具有判别力的节点集合。**我们将这种几个关节点同时影响和决定判别的特性称为共现性（Co-occurrence）**  

&emsp; 海康威视这篇文章做的事情是从人体关键点序列进行行为检测和识别,骨架的相互作用和组合在描述动作特征上共同发挥了关键性作用。文中主要提出了全局共现特征的一种提取方法,即通过转置关键点序列,将被卷积的通道维度由坐标维度变为关键点维度,从而提取关键点的全局共现特征;除了全局共现特征,文中还提到了关于多人交互动作问题的解决方法;最后也借鉴Faster RCNN的proposal思想,提出了一种时域动作检测方法.
### 局部（点）和全局卷积特征
![Decomposition](https://github.com/CSer-Tang-hao/Papers-Reading-Recording/blob/master/IJCAI2018/img/Decomposition.png)  
&emsp; 尽管 LSTM 网络就是为建模长期的时间依赖关系而设计的，但由于时间建模是在原始输入空间上完成的，所以它们难以直接从骨架上学习到高层面的特征.而全连接层则有能力聚合所有输入神经元的全局信息，进而可以学习到共现特征.文章设计了一种全新的端到端分层式特征学习网络，其中的特征是从点层面特征到全局共现特征逐渐聚合起来的
&emsp; **3×3 卷积的分解分为两个步骤:**  
&emsp; (a) 每个输入通道的空间域中的独立 2D 卷积，其中的特征是从 3×3 的临近区域局部聚合的 
&emsp; (b) 各个通道上逐个元素求和，其中的特征是在所有输入通道上全局地聚合  
&emsp;<a href="https://www.codecogs.com/eqnedit.php?latex=T\times&space;N\times&space;D" target="_blank"><img src="https://latex.codecogs.com/gif.latex?T\times&space;N\times&space;D" title="T\times N\times D" /></a> 的Feature Map过两个卷积层,卷积层参数分别是<a href="https://www.codecogs.com/eqnedit.php?latex=\left&space;(&space;1\times1\times64&space;\right&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\left&space;(&space;1\times1\times64&space;\right&space;)" title="\left ( 1\times1\times64 \right )" /></a>和<a href="https://www.codecogs.com/eqnedit.php?latex=\left&space;(&space;3\times1\times32&space;\right&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\left&space;(&space;3\times1\times32&space;\right&space;)" title="\left ( 3\times1\times32 \right )" /></a>. Feature Map结构为<a href="https://www.codecogs.com/eqnedit.php?latex=\left&space;(H,W,C&space;\right&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\left&space;(H,W,C&space;\right&space;)" title="\left (H,W,C \right )" /></a>,卷积核<a href="https://www.codecogs.com/eqnedit.php?latex=\left&space;(1\times1&space;\right&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\left&space;(1\times1&space;\right&space;)" title="\left (1\times1 \right )" /></a>和<a href="https://www.codecogs.com/eqnedit.php?latex=\left&space;(3\times1&space;\right&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\left&space;(3\times1&space;\right&space;)" title="\left (3\times1 \right )" /></a>分别是关键点级(point-level)的特征和3帧时序上的关键点级特征.  
&emsp; 通过tranpose(permute)操作,按(0,2,1)将Feature Map转为<a href="https://www.codecogs.com/eqnedit.php?latex=T\times&space;D\times&space;N" target="_blank"><img src="https://latex.codecogs.com/gif.latex?T\times&space;D\times&space;N" title="T\times D\times N" /></a>顺序.使用两层<a href="https://www.codecogs.com/eqnedit.php?latex=3\times&space;3" target="_blank"><img src="https://latex.codecogs.com/gif.latex?3\times&space;3" title="3\times 3" /></a>卷积核作用其上(原文中filter数分别为32和64,stride为2),N个关键点作为channel,卷积层可提取所有关键点间的全局特征.<a href="https://www.codecogs.com/eqnedit.php?latex=3\times&space;3" target="_blank"><img src="https://latex.codecogs.com/gif.latex?3\times&space;3" title="3\times 3" /></a>卷积核在对应3帧和3个维度的坐标上.  
### 多人问题  
**为什么？** 
部分动作如拥抱、握手等，是多人的  
**怎么做？**  
**早期融合:** 多人的关键点信息在输入网络前进行stack,设置最大可接受人数,人数不足时,zero padding  
**后期融合:** 多人的关键点信息分别输入网络,取conv6的特征,进行Max,Mean或Concat操作后,再进行分类
![Multi-person](https://github.com/CSer-Tang-hao/Papers-Reading-Recording/blob/master/IJCAI2018/img/Multi-person.png),根据**Core nerwork**的内部状态<a href="https://www.codecogs.com/eqnedit.php?latex=h_{T}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h_{T}" title="h_{T}" /></a>开始输出prediction  
&emsp; **亮点**:文章基于深度强化学习的方法训练了一个智能体(Agent),这个智能体可以measuring confidence score进行判断与行动，对视频中一些不具判断信息的帧进行快进与跳过，对一些informative frame就slow down.快进与慢退的size也依靠Agent自主选择，当confident enough,Agent自行停止watching开始分类，这种方法的构思仿照人类判断的过程，并不需要看完整个video,靠一些key frame就可以做出分类判断，这样就可以Lower the number of processed frame，从而提高
分类的效率。  
&emsp; ArXiv：https://arxiv.org/abs/1804.06055
&emsp; Code：https://github.com/fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition （Keras）  
&emsp; Code：https://github.com/zhujiagang/co-cn （Pytorch）  
&emsp; Datasets: **YouTobe-8M**  
&emsp; 注：之前看过16年的一篇CVPR"End to end learning of action detection from frame glimpses in videos",是用训练好的agent去做action detection,应该是近几年CV顶会上第一篇把**RL**用到Video& Action上，发现这篇IJCAI的构思与写法都是模仿了16年的CVPR,区别在于IJCAI **limit actions to a local range**,CVPR2016 **have no constraint on the action**  
&emsp; Idea：这篇文章的“快进”、“倒退”思路让我想起了梯度下降法寻找最低点也是类似的过程，CVPR2018有篇文章就是使用自控的PID控制器改造了SGD，取得了较优的效果，这篇文章只是固定了一些“快进”与“慢退”的步长，如果加入了PID让这些步长不受约束，更加精细化，将PID控制器的反馈加入Agent的奖励与惩罚中，提高效率的话也可以先做一些简单的proposal再做classification,有时间将这些方法用在Untrimmed video 的classification & localization试一试
